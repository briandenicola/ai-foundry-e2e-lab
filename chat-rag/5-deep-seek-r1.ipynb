{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a3d8f7b1",
   "metadata": {},
   "source": [
    "# üöÄ DeepSeek-R1 Model with Azure AI Inference üß†\n",
    "\n",
    "**DeepSeek-R1** is a state-of-the-art reasoning model combining reinforcement learning and supervised fine-tuning, excelling at complex reasoning tasks with 37B active parameters and 128K context window.\n",
    "\n",
    "In this notebook, you'll learn to:\n",
    "1. **Initialize** the ChatCompletionsClient for Azure serverless endpoints\n",
    "2. **Chat** with DeepSeek-R1 using reasoning extraction\n",
    "3. **Implement** a travel planning example with step-by-step reasoning\n",
    "4. **Leverage** the 128K context window for complex scenarios\n",
    "\n",
    "## Why DeepSeek-R1?\n",
    "- **Advanced Reasoning**: Specializes in chain-of-thought problem solving\n",
    "- **Massive Context**: 128K token window for detailed analysis\n",
    "- **Efficient Architecture**: 37B active parameters from 671B total\n",
    "- **Safety Integrated**: Built-in content filtering capabilities\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6e3a4c2",
   "metadata": {},
   "source": [
    "## 1. Setup & Authentication\n",
    "\n",
    "This notebook uses the same authentication pattern as other workshop notebooks:\n",
    "- `azure-ai-projects`: For AIProjectClient integration\n",
    "- `azure-identity`: For InteractiveBrowserCredential authentication\n",
    "\n",
    ".env file requirements (same as other notebooks):\n",
    "```bash\n",
    "PROJECT_CONNECTION_STRING=<your-project-endpoint-url>\n",
    "TENANT_ID=<your-azure-tenant-id>\n",
    "```\n",
    "\n",
    "### DeepSeek-R1 Model Setup\n",
    "- **Model Deployment Name**: `DeepSeek-R1-0528`\n",
    "- Make sure this model is deployed in your Azure AI Foundry project\n",
    "- The model will be accessed through the project client using browser authentication\n",
    "\n",
    "> **Note**: This notebook uses the standardized authentication pattern from other workshop notebooks, eliminating the need for API keys and using browser-based authentication instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a53f8d4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîë Using Tenant ID: ed244546-f48e-4572-a767-d6d2a521a7c5\n",
      "ü§ñ Using model: DeepSeek-R1-0528\n",
      "üåê Using browser-based authentication to bypass Azure CLI cache issues...\n",
      "‚úÖ AIProjectClient created successfully!\n",
      "üéâ DeepSeek-R1 client is ready!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "from dotenv import load_dotenv\n",
    "from pathlib import Path\n",
    "from azure.ai.projects import AIProjectClient\n",
    "from azure.identity import InteractiveBrowserCredential\n",
    "from azure.ai.inference.models import SystemMessage, UserMessage\n",
    "\n",
    "# Load environment variables (same pattern as other workshop notebooks)\n",
    "notebook_path = Path().absolute()\n",
    "parent_dir = notebook_path.parent\n",
    "load_dotenv(parent_dir / '.env')\n",
    "\n",
    "# Get workshop-standard environment variables\n",
    "project_endpoint = os.getenv(\"PROJECT_CONNECTION_STRING\")\n",
    "tenant_id = os.getenv(\"TENANT_ID\")\n",
    "model_name = \"DeepSeek-R1-0528\"  # DeepSeek model deployment name\n",
    "\n",
    "print(f\"üîë Using Tenant ID: {tenant_id}\")\n",
    "print(f\"ü§ñ Using model: {model_name}\")\n",
    "\n",
    "# Initialize client with InteractiveBrowserCredential (same as other notebooks)\n",
    "try:\n",
    "    print(\"üåê Using browser-based authentication to bypass Azure CLI cache issues...\")\n",
    "    \n",
    "    # Use only InteractiveBrowserCredential with the specific tenant\n",
    "    credential = InteractiveBrowserCredential(tenant_id=tenant_id)\n",
    "    \n",
    "    # Create the project client using endpoint\n",
    "    project_client = AIProjectClient(\n",
    "        endpoint=project_endpoint,\n",
    "        credential=credential\n",
    "    )\n",
    "    \n",
    "    print(\"‚úÖ AIProjectClient created successfully!\")\n",
    "    print(\"üéâ DeepSeek-R1 client is ready!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(\"‚ùå Initialization failed:\", e)\n",
    "    print(\"üí° Please complete the browser authentication prompt that should appear\")\n",
    "    print(\"\\nüìã Required .env file format:\")\n",
    "    print(\"PROJECT_CONNECTION_STRING=<your-project-endpoint>\")\n",
    "    print(\"TENANT_ID=<your-tenant-id>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f07bdf15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü§ñ Target model: DeepSeek-R1-0528\n",
      "üîÑ Fallback model available: gpt-4.1-mini\n",
      "üí° If DeepSeek-R1 isn't deployed, we'll use the fallback model\n",
      "\n",
      "üöÄ Ready to demonstrate DeepSeek-R1's reasoning capabilities!\n"
     ]
    }
   ],
   "source": [
    "# Model fallback logic (same as other workshop notebooks)\n",
    "print(f\"ü§ñ Target model: {model_name}\")\n",
    "\n",
    "# Check if we have a fallback model available\n",
    "fallback_model = os.getenv(\"MODEL_DEPLOYMENT_NAME\")\n",
    "if fallback_model and fallback_model != model_name:\n",
    "    print(f\"üîÑ Fallback model available: {fallback_model}\")\n",
    "    print(f\"üí° If DeepSeek-R1 isn't deployed, we'll use the fallback model\")\n",
    "    # Note: In case of 404 errors, you can manually set: model_name = fallback_model\n",
    "else:\n",
    "    print(f\"‚úÖ Using specified model: {model_name}\")\n",
    "\n",
    "print(\"\\nüöÄ Ready to demonstrate DeepSeek-R1's reasoning capabilities!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fac7403f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3c01d5d9",
   "metadata": {},
   "source": [
    "## 2. Intelligent Travel Planning ‚úàÔ∏è\n",
    "\n",
    "Demonstrate DeepSeek-R1's reasoning capabilities for trip planning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e6a5d8d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üó∫Ô∏è Query: Plan a 5-day cultural trip to Kyoto in April\n",
      "\n",
      "üìù Travel Plan:\n",
      "Here‚Äôs a meticulously crafted 5-day Kyoto cultural itinerary for April, blending iconic sights with authentic hidden gems, prioritizing crowd management, and incorporating essential safety tips:\n",
      "\n",
      "**Key Considerations for April:**\n",
      "*   **Cherry Blossom (Sakura) Season:** Peak bloom varies (late March-early April). Expect **major crowds** and higher prices. Book **everything** (flights, hotels, popular restaurants, specific attractions) **months in advance**.\n",
      "*   **Weather:** Mild days (10-20¬∞C / 50-68¬∞F), cool evenings. Pack layers, a light waterproof jacket, and **comfortable walking shoes** (essential!).\n",
      "*   **Safety:** Kyoto is very safe. Primary concerns are **pickpockets in crowded areas** (markets, temples during peak bloom), **traffic** (watch for bikes/scooters on narrow streets), and **slippery surfaces** (old stone paths, temple floors). Stay hydrated.\n",
      "\n",
      "**The Itinerary: Balancing Icons & Serenity**\n",
      "\n",
      "**Day 1: Arrival & Gion's Allure**\n",
      "*   **Morning/Afternoon:** Arrive at Kansai International Airport (KIX) or Osaka Itami (ITM). Take the Haruka Express train or limousine bus directly to Kyoto Station.\n",
      "*   **Late Afternoon:** Check into accommodation (Central Kyoto/Gion/Higashiyama areas are ideal). **Safety:** Use official taxi stands; ensure meter is running.\n",
      "*   **Evening (Cultural Immersion):**\n",
      "    *   **Iconic:** Explore **Gion** at dusk. Spot traditional machiya houses. *Rationale:* Best time to feel the atmosphere before crowds peak.\n",
      "    *   **Hidden Gem:** Visit **Yasaka Shrine (Gion Corner)**. Less intense than daytime. Then, walk down **Shirakawa Minami-dori** (canal street south of Gion) ‚Äì beautifully lit, quieter than Hanami-koji. *Rationale:* Avoids the main tourist throngs while capturing Gion's essence.\n",
      "    *   **Deeper Gem:** **Kennin-ji Temple** (Oldest Zen temple in Kyoto). Open until 5 PM, often quieter later. Stunning dragon ceiling & gardens. *Rationale:* Offers serenity and profound culture near the bustle.\n",
      "*   **Dinner:** Try Kaiseki (multi-course) in Pontocho (book ahead) or casual eats at Nish\n"
     ]
    }
   ],
   "source": [
    "def plan_trip_with_reasoning(query, show_thinking=False):\n",
    "    \"\"\"Get travel recommendations with reasoning extraction\"\"\"\n",
    "    try:\n",
    "        messages = [\n",
    "            SystemMessage(content=\"You are a travel expert. Provide detailed plans with rationale.\"),\n",
    "            UserMessage(content=f\"{query} Include hidden gems and safety considerations.\")\n",
    "        ]\n",
    "        \n",
    "        # Use the project client's Azure OpenAI client with proper API\n",
    "        with project_client.get_openai_client(api_version=\"2024-10-21\") as chat_client:\n",
    "            response = chat_client.chat.completions.create(\n",
    "                messages=messages,\n",
    "                model=model_name,\n",
    "                temperature=0.7,\n",
    "                max_tokens=1024\n",
    "            )\n",
    "        \n",
    "        content = response.choices[0].message.content\n",
    "        \n",
    "        # Extract reasoning if present (DeepSeek-R1 specific)\n",
    "        if show_thinking and \"<thinking>\" in content and \"</thinking>\" in content:\n",
    "            thinking_start = content.find(\"<thinking>\") + 10\n",
    "            thinking_end = content.find(\"</thinking>\")\n",
    "            thinking = content[thinking_start:thinking_end].strip()\n",
    "            final_answer = content[thinking_end + 12:].strip()\n",
    "            \n",
    "            return {\n",
    "                \"thinking\": thinking,\n",
    "                \"answer\": final_answer,\n",
    "                \"full_response\": content\n",
    "            }\n",
    "        \n",
    "        return content\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error calling API: {e}\")\n",
    "        print(f\"üí° Make sure the model '{model_name}' is properly deployed\")\n",
    "        return None\n",
    "\n",
    "# Example usage\n",
    "query = \"Plan a 5-day cultural trip to Kyoto in April\"\n",
    "result = plan_trip_with_reasoning(query, show_thinking=True)\n",
    "\n",
    "print(\"üó∫Ô∏è Query:\", query)\n",
    "if isinstance(result, dict):\n",
    "    print(\"\\nüß† Model's thinking process:\")\n",
    "    print(result[\"thinking\"])\n",
    "    print(\"\\nüìù Final recommendation:\")\n",
    "    print(result[\"answer\"])\n",
    "elif result:\n",
    "    print(\"\\nüìù Travel Plan:\")\n",
    "    print(result)\n",
    "else:\n",
    "    print(\"‚ùå Failed to get travel recommendations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d8f1b3a",
   "metadata": {},
   "source": [
    "## 3. Technical Problem Solving üíª\n",
    "\n",
    "Showcase coding/optimization capabilities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e5d4a3e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß Problem: \n",
      "Optimize this Python function for finding the maximum subarray sum:\n",
      "\n",
      "def max_subarray_naive(arr):\n",
      "    max_sum = float('-inf')\n",
      "    for i in range(len(arr)):\n",
      "        for j in range(i, len(arr)):\n",
      "            current_sum = sum(arr[i:j+1])\n",
      "            max_sum = max(max_sum, current_sum)\n",
      "    return max_sum\n",
      "\n",
      "The current time complexity is O(n¬≥). Can you improve it?\n",
      "\n",
      "\n",
      "‚öôÔ∏è Solution:\n",
      "To optimize the given function for finding the maximum subarray sum, we can replace the naive O(n¬≥) approach with Kadane's algorithm, which efficiently computes the solution in O(n) time with O(1) space complexity. Here's the improved implementation:\n",
      "\n",
      "```python\n",
      "def max_subarray(arr):\n",
      "    if not arr:\n",
      "        return 0\n",
      "    max_current = max_global = arr[0]\n",
      "    for num in arr[1:]:\n",
      "        max_current = max(num, max_current + num)\n",
      "        if max_current > max_global:\n",
      "            max_global = max_current\n",
      "    return max_global\n",
      "```\n",
      "\n",
      "**Explanation:**\n",
      "1. **Initialization:** The algorithm initializes two variables, `max_current` and `max_global`, to the first element of the array. These track the maximum sum ending at the current position and the overall maximum sum found, respectively.\n",
      "2. **Iteration:** For each subsequent element in the array:\n",
      "   - **Update `max_current`:** This is set to the maximum of the current element itself or the sum of the current element and `max_current`. This step decides whether to start a new subarray at the current element or extend the previous subarray.\n",
      "   - **Update `max_global`:** If `max_current` exceeds `max_global`, update `max_global` to this new maximum.\n",
      "3. **Edge Handling:** If the input array is empty, the function returns 0 (handled by the initial check).\n",
      "\n",
      "**Key Improvements:**\n",
      "- **Time Complexity:** Reduced from O(n¬≥) to O(n) by processing each element only once.\n",
      "- **Space Complexity:** Reduced to O(1) as only a few variables are used, regardless of input size.\n",
      "\n",
      "This approach efficiently handles all cases, including arrays with all negative numbers, by ensuring the maximum subarray sum is correctly identified without unnecessary recomputation.\n",
      "\n",
      "‚öôÔ∏è Solution:\n",
      "To optimize the given function for finding the maximum subarray sum, we can replace the naive O(n¬≥) approach with Kadane's algorithm, which efficiently computes the solution in O(n) time with O(1) space complexity. Here's the improved implementation:\n",
      "\n",
      "```python\n",
      "def max_subarray(arr):\n",
      "    if not arr:\n",
      "        return 0\n",
      "    max_current = max_global = arr[0]\n",
      "    for num in arr[1:]:\n",
      "        max_current = max(num, max_current + num)\n",
      "        if max_current > max_global:\n",
      "            max_global = max_current\n",
      "    return max_global\n",
      "```\n",
      "\n",
      "**Explanation:**\n",
      "1. **Initialization:** The algorithm initializes two variables, `max_current` and `max_global`, to the first element of the array. These track the maximum sum ending at the current position and the overall maximum sum found, respectively.\n",
      "2. **Iteration:** For each subsequent element in the array:\n",
      "   - **Update `max_current`:** This is set to the maximum of the current element itself or the sum of the current element and `max_current`. This step decides whether to start a new subarray at the current element or extend the previous subarray.\n",
      "   - **Update `max_global`:** If `max_current` exceeds `max_global`, update `max_global` to this new maximum.\n",
      "3. **Edge Handling:** If the input array is empty, the function returns 0 (handled by the initial check).\n",
      "\n",
      "**Key Improvements:**\n",
      "- **Time Complexity:** Reduced from O(n¬≥) to O(n) by processing each element only once.\n",
      "- **Space Complexity:** Reduced to O(1) as only a few variables are used, regardless of input size.\n",
      "\n",
      "This approach efficiently handles all cases, including arrays with all negative numbers, by ensuring the maximum subarray sum is correctly identified without unnecessary recomputation.\n"
     ]
    }
   ],
   "source": [
    "def solve_technical_problem(problem):\n",
    "    \"\"\"Solve complex technical problems with structured reasoning\"\"\"\n",
    "    try:\n",
    "        with project_client.get_openai_client(api_version=\"2024-10-21\") as chat_client:\n",
    "            response = chat_client.chat.completions.create(\n",
    "                messages=[\n",
    "                    UserMessage(content=f\"{problem} Please reason step by step, and put your final answer within \\\\boxed{{}}.\")\n",
    "                ],\n",
    "                model=model_name,\n",
    "                temperature=0.3,\n",
    "                max_tokens=2048\n",
    "            )\n",
    "        \n",
    "        return response.choices[0].message.content\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error solving problem: {e}\")\n",
    "        return None\n",
    "\n",
    "# Example: Algorithm optimization\n",
    "problem = \"\"\"\n",
    "Optimize this Python function for finding the maximum subarray sum:\n",
    "\n",
    "def max_subarray_naive(arr):\n",
    "    max_sum = float('-inf')\n",
    "    for i in range(len(arr)):\n",
    "        for j in range(i, len(arr)):\n",
    "            current_sum = sum(arr[i:j+1])\n",
    "            max_sum = max(max_sum, current_sum)\n",
    "    return max_sum\n",
    "\n",
    "The current time complexity is O(n¬≥). Can you improve it?\n",
    "\"\"\"\n",
    "\n",
    "print(\"üîß Problem:\", problem)\n",
    "solution = solve_technical_problem(problem)\n",
    "if solution:\n",
    "    print(\"\\n‚öôÔ∏è Solution:\")\n",
    "    print(solution)\n",
    "else:\n",
    "    print(\"‚ùå Failed to get solution\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b9f7a8c",
   "metadata": {},
   "source": [
    "## 4. Best Practices & Considerations\n",
    "\n",
    "1. **Reasoning Handling**: Use regex to separate <think> content from final answers\n",
    "2. **Safety**: Built-in content filtering - handle HttpResponseError for violations\n",
    "3. **Performance**:\n",
    "   - Max tokens: 4096\n",
    "   - Rate limit: 200K tokens/minute\n",
    "4. **Cost**: Pay-as-you-go with serverless deployment\n",
    "5. **Streaming**: Implement response streaming for long completions\n",
    "\n",
    "```python\n",
    "# Streaming example\n",
    "response = client.chat.completions.create(..., stream=True)\n",
    "for chunk in response:\n",
    "    print(chunk.choices[0].delta.content or \"\", end=\"\")\n",
    "```\n",
    "\n",
    "## üéØ Key Takeaways\n",
    "- Leverage 128K context for detailed analysis\n",
    "- Extract reasoning steps for debugging/analysis\n",
    "- Combine with Azure AI Content Safety for production\n",
    "- Monitor token usage via response.usage\n",
    "\n",
    "> Always validate model outputs for critical applications!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

